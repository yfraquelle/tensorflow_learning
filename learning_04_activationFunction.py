# activation function
# tf.nn.relu(features, name=None) recv<0,ouput=0;recv>0,output linearly
# tf.nn.relu6(features, name=None)
# tf.nn.crelu(features, name=None)
# tf.nn.elu(features, name=None) 
# tf.nn.softplus(features, name=None) classification
# tf.nn.softsign(features, name=None) classification
# tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) classification
# tf.nn.bias_add(value, bias, data_format=None, name=None) reduce the overfitting problem
# tf.sigmoid(x, name=None) classification 
# tf.tanh(x, name=None) 
